\chapter{Literature Review}

\section{Background}

Rapid digitalisation has increased the scale at which organisations collect and process personal data, making data protection a key concern, especially in Singapore \parencite{zul2022}.
In response to these challenges, the PDPA was established to safeguard the personal data of individuals while supporting the business interests of organisations \parencite{pdpc2023}.
To aid compliance, the PDPC publishes advisory guidelines to help organisations and individuals understand the PDPA \parencite{pdpc2022}.

\section{Challenges}

Despite the availability of these resources, many organisations and individuals struggle to understand and comply with PDPA requirements correctly because regulations continually evolve \parencite{ismart2024}.
According to the PDPC's 2015 industry survey, about 58\% of organisations required support to achieve compliance, reflecting knowledge and resource gaps \parencite{pdpc2015}.
Additionally, there is growing interest in leveraging AI to automate the retrieval of knowledge and comprehension of regulatory compliance \parencite{gultekinvarkonyi2025}.
There is, therefore, an opportunity to combine AI and compliance to create tools that can interpret legal frameworks and make regulatory knowledge more accessible.

SMEs form the backbone of Singapore's economy \parencite{lim2025}.
However, SMEs often face greater obstacles in meeting compliance requirements compared to larger organisations due to limited financial resources and the growing complexity of regulatory obligations \parencite{bello2024}.
As a result, many do not allocate sufficient time, budget, or staff to interpret and implement compliance regulations \parencite{complianceconsultant2025}.
Digital capability gaps further worsen the compliance challenge.
According to a survey conducted by Capterra, 32\% of SMEs still rely on spreadsheets to manage customer information, while another 35\% use manual methods or email communication, which are insufficient under modern data protection guidelines \parencite{navarrete2019}.
These informal, decentralised data management methods make it difficult to track consent, accurately update records, and securely ensure retention and disposal.
In this context, there is a clear need for accessible, low-barrier compliance support tools tailored to SMEs.
Solutions such as AI-driven PDPA chatbots can lower the knowledge and resource barrier by providing SMEs with immediate, accurate guidance without requiring legal expertise, formal training, or expensive consultancy services.

\section{Solutions}

A variety of resources are available to support PDPA compliance, primarily provided by the PDPC.
These include advisory guidelines, compliance checklists, and the Data Protection Essentials programme for SMEs \parencite{pdpc2025kickstart}.
However, users often struggle to identify which specific sections apply to their situation or to interpret PDPA requirements without legal or technical expertise \parencite{lonzetta2020}.

In addition to PDPC materials, several commercial and professional solutions are available.
Many organisations engage outsourced Data Protection Officers or legal consultants to interpret PDPA requirements on their behalf.
Others adopt enterprise-grade privacy management systems such as Varonis, which offer data governance dashboards and risk assessment modules.
However, these systems are costly, and they require ongoing subscription fees, which are impractical for SMEs with limited budgets.

\subsection{Gaps}

However, existing solutions lack conversational, real-time interaction.
Users must manually search through documentation, navigate dashboards, or rely on external consultants.
Taken together, these findings highlight the absence of an accessible, affordable, and PDPA-specific digital tool capable of providing real-time compliance support.
This gap underscores the need for an AI-powered PDPA chatbot that allows users to obtain accurate, scenario-specific guidance without requiring legal expertise or substantial financial investment.

The increasing complexity of legal and regulatory frameworks has driven organisations to adopt AI solutions to automate and streamline compliance processes \parencite{bleach2024}.
LLMs allow systems to interpret, analyse, and generate human-like responses to text-based queries \parencite{vaniukov2024}.
In the legal industry, AI tools are being used for document analysis, legal advice support, and contract drafting, reducing mundane work and improving efficiency \parencite{smu_nd}.
A key innovation in this area is AI chatbots, which act as conversational agents to answer user queries about legal and compliance matters.
In the legal industry, AI chatbots have already begun transforming how professionals access and interpret information.
For instance, Harvey AI, developed on OpenAI's Generative Pre-trained Transformer (GPT) technology, has partnered with law firms and consulting giants such as PwC to assist in legal research, contract review, and compliance analysis \parencite{pwc2023}.
This growing adoption highlights the potential for AI to streamline legal workflows and enhance user understanding of complex legal texts.
Similarly, there is growing recognition that such tools can extend to the data protection and compliance domain.
Developments in this area align with Smart Nation initiatives, which encourage the use of AI to improve governance, productivity, and security \parencite{govsg2023}.
Thus, AI-powered PDPA chatbots represent a promising approach to bridging compliance gaps, especially for resource-constrained SMEs.

AI chatbots present several privacy and data protection risks, particularly when deployed in compliance-related contexts.
Large language models generate outputs probabilistically and may produce inaccurate, misleading, or hallucinated information.
OpenAI publicly acknowledges this behaviour in its model documentation, noting that such models can generate plausible-sounding but incorrect statements \parencite{openai2023}.
In regulatory settings, such inaccuracies carry amplified consequences because organisations may unintentionally rely on erroneous interpretations when making compliance decisions.
Established research in human-automation interaction also shows that users tend to over-trust automated systems, especially when the system appears competent and fluent, which increases the likelihood that incorrect responses are accepted without critical evaluation \parencite{goddard2012}.

A second major concern relates to how chatbots handle user inputs.
If systems are not designed with strict minimisation or protection controls, sensitive information may be captured through server logs, analytics tools, or external telemetry.
PDPC guidance emphasises that the handling of identifiers---such as National Registration Identity Card (NRIC) numbers---involves heightened obligations due to the risks of identity theft and misuse if the data is retained or transmitted unnecessarily \parencite{pdpc2022}.
Cisco's 2023 Data Privacy Benchmark Study also reports that users frequently submit sensitive information into digital tools without fully understanding how it will be stored or processed, increasing organisations' responsibility to implement strong safeguards \parencite{cisco2023}.

Jurisdictional accuracy is another key challenge.
General-purpose LLMs often incorporate patterns from global datasets, which can lead to inappropriate blending of regulatory concepts across jurisdictions.
The Organisation for Economic Co-operation and Development (OECD) warns that AI systems trained on mixed-jurisdiction data may unintentionally provide legal interpretations that do not align with local laws, creating significant compliance risks \parencite{oecd2023}.

Finally, AI chatbots can amplify misinformation risks.
IBM's research on trustworthy AI highlights that the natural language fluency of modern LLMs can obscure underlying inaccuracies, making hallucinations especially dangerous in high-stakes domains such as law, healthcare, and compliance \parencite{chen2023}.
These risks reinforce the need for strict data-minimisation practices, well-defined refusal behaviour, jurisdiction-specific safeguards, and transparent user guidance to ensure that AI chatbots support---rather than undermine---organisational compliance efforts.

\subsection{Frontiers}

Retrieval-augmented generation is an increasingly important technique for improving the factual accuracy, grounding, and explainability of LLM-based systems.
Rather than relying solely on internal model parameters, RAG retrieves relevant information from an external knowledge base and incorporates it into the model's context before generating an output \parencite{lewis2020}.
This approach helps reduce hallucinations by tying responses to verifiable documents rather than latent model associations.
RAG is particularly well suited for compliance and regulatory domains.
By grounding responses in authoritative materials---such as PDPC advisory guidelines, enforcement decisions, and published regulatory notes---it ensures that outputs remain aligned with official interpretations and reduces the risk of misleading or inconsistent advice \parencite{pdpc2022}.
Industry research has also shown that retrieval-based grounding improves factual reliability in enterprise AI systems, especially in legal and knowledge-intensive applications where accuracy is critical \parencite{packowski2024}.
Jurisdictional control is another key advantage.
General-purpose LLMs may generate guidance influenced by foreign regulatory frameworks such as the General Data Protection Regulation (GDPR), which may be inappropriate in the Singapore context.
RAG mitigates this risk by restricting the retrieval corpus to Singapore-specific PDPA documents, ensuring that responses remain locally relevant and legally consistent \parencite{oecd2023}.
RAG also supports maintainability.
Because the external knowledge base can be updated independently of the model, new PDPC guidelines, decisions, or advisories can be integrated without requiring model retraining \parencite{lewis2020}.
This modularity makes RAG particularly suitable for regulatory settings where requirements evolve over time.
Overall, RAG provides a robust foundation for AI-assisted PDPA guidance by combining the generative flexibility of LLMs with the factual grounding necessary for compliance-critical tasks.

\subsection{Ethics}

AI systems designed for compliance must align with core PDPA obligations---including Purpose Limitation, Accuracy, Protection, and Retention Limitation---which require organisations to minimise the collection of personal data, implement appropriate safeguards, and ensure that information used to support decisions is accurate and fit for purpose \parencite{pdpc2022}.
Privacy-by-design principles guide the system's architecture.
User inputs are processed transiently, with no persistent server-side logging of identifiers.
Where optional chat history is enabled, records are stored locally on the user's browser rather than transmitted to the backend.
Following PDPC recommendations, the interface also includes guidance discouraging users from submitting NRIC numbers or other sensitive identifiers, reducing the risk of inadvertent data collection \parencite{pdpc2022}.

Research stresses the importance of technical guardrails---such as retrieval-based grounding, domain-specific prompting, and conservative generation settings---to mitigate misleading or non-jurisdictional outputs \parencite{chen2023}.
The platform incorporates controlled refusal behaviour for high-risk queries, including those involving identifiable personal data or legal questions outside Singapore's context.
Responsible AI literature also highlights techniques such as validated knowledge sources, rate limiting, and transparency prompts as effective mitigation strategies in safety-critical systems \parencite{deepmind2024}.
These principles inform the system's design, which limits data retention, enforces strict query filtering, and provides users with clear explanations of system scope and limitations.
If future features---such as server-side chat history, organisational analytics, or multi-user administration---are introduced, additional safeguards will be required.
These include encryption at rest, role-based access controls, access logging, and explicit retention and deletion policies to remain compliant with PDPA obligations \parencite{pdpc2022}.
