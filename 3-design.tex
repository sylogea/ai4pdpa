\chapter{System Design}

To support our goal of helping Singaporean SMEs comply with the PDPA, we design a generation pipeline consisting of three steps: moderation, retrieval, and generation.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{images/pipeline.png}
	\caption{Pipeline representation of the system.}
\end{figure}

\vspacebaselineskip

Given a user query, the moderation step checks whether the query is safe to handle, the retrieval step finds information relevant to the query, and the generation step crafts a response to the query.
On a high level, our system consists of a frontend, backend, and infrastructure.
The frontend handles the exposed logic, the backend handles the non-exposed logic, while the infrastructure hosts the backend and frontend.

\section{Backend}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{images/backend-3.png}
	\caption{Graphical representation of the backend.}
\end{figure}

\vspacebaselineskip

To ensure factuality, the retrieval step only retrieves information from documents defined as truthful.
To ensure transparency, the generation step names each source from which information is retrieved.

Given a set of documents, the system performs the following steps offline.

\begin{enumerate}
	\item \textbf{Chunking.} The system divides each document into smaller pieces called ``chunks''.
	This is performed to make downstream information consumption more manageable.
	\item \textbf{Embedding.} The system maps each chunk to a numerical representation capturing the semantics of the chunk, called an ``embedding''.
	This is performed to allow for downstream semantic comparisons with user queries.
	\item \textbf{Indexing.} The system stores each chunk and its corresponding embedding in a store optimised for embedding retrieval.
	This is performed to allow for efficient downstream retrieval of relevant chunks for a user query.
\end{enumerate}

Given a user query, the system performs the following steps online.

\begin{enumerate}
	\item \textbf{Moderation.} The system checks if the query seems safe to handle.
	If it seems unsafe, the pipeline returns a generic response (e.g. ``Sorry, I cannot help with that.'') to the user; otherwise, the pipeline moves to the next step.
	This is performed to protect the user and our system from any adverse effects that might result from sending an unsafe query to the chat model.
	\item \textbf{Retrieval.} The system finds the most relevant chunks to the query by computing a similarity score between the query and each chunk, and returns the highest-scoring chunks.
	This is performed to retrieve relevant information for the query.
	\item \textbf{Generation.} The system queries the chat model with the conversation, query, and chunks, and returns the response of the model.
\end{enumerate}

We had to make several design decisions for our vector store, inference provider, embedding model, and chat model.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{Chroma} & \textbf{Pinecone} & \textbf{Qdrant} \\
		\midrule
		\textbf{License class} & Open & Closed & Open \\
		\textbf{GitHub stars} & $\sim$25k & $\sim$3k & $\sim$27k \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between Chroma, Pinecone, and Qdrant.}
\end{table}

\vspacebaselineskip

From our research, we found 3 vector store abstractions: Chroma, Pinecone, and Qdrant.
We settled on Qdrant as the open license class gives greater developmental control, and the larger community support---as measured by the number of GitHub stars---facilitates debugging.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{GroqCloud} & \textbf{OpenRouter} & \textbf{Vertex AI} \\
		\midrule
		\textbf{Relative model count} & Smallest & Largest & In-between \\
		\textbf{Embedding model support?} & No & Yes & Yes \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between GroqCloud, OpenRouter, and Vertex AI.}
\end{table}

\vspacebaselineskip

From our research, we found three inference provider abstractions: GroqCloud, OpenRouter, and Vertex AI.
We settled on OpenRouter as its model support provides the greatest developmental flexibility.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{TE3-Small} & \textbf{BGE-M3} & \textbf{QE-8B} \\
		\midrule
		\textbf{Price} & \$0.02/M tokens & \$0.01/M tokens & \$0.01/M tokens \\
		\textbf{Context size} & 8,192 & 8,192 & 32,000 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between TE3-Small, BGE-M3, and QE-8B.}
\end{table}

\vspacebaselineskip

For our embedding model, we wanted a model with a context size greater than 512 tokens for input flexibility.
Sorting the models officially supported by OpenRouter with that property by price, the top three models were OpenAI's Text Embedding 3 Small (TE3-Small), BAAI's BGE-M3 (BGE-M3), and Qwen's Qwen3 Embedding 8B (QE-8B).
We settled on QE-8B, as the higher context size gives input flexibility.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{gpt-5-chat} & \textbf{gemini-2.5-flash} & \textbf{llama-4-maverick} \\
		\midrule
		\textbf{Price} & \$11.25/M tokens & \$2.80/M tokens & \$0.75/M tokens \\
		\textbf{Average latency} & 0.59s & 0.88s & 0.34s \\
		\textbf{Context size} & 128,000 tokens & 1,024,576 tokens & 1,024,576 tokens \\
		\textbf{Input modalities} & Text, image, file & Text, image, file, audio, video & Text, image \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between gpt-5-chat, gemini-2.5-flash, and llama-4-maverick.}
\end{table}

\vspacebaselineskip

For our chat model, we wanted a model with
an average latency within 1 second and throughput no less than 48 tokens per second for responsiveness,
a context size no less than 128,000 tokens for in-conversation memory,
input modalities including text and image for input flexibility, and
a maximum output size no less than 8,000 tokens for output flexibility.
Sorting the models officially supported by OpenRouter with those properties by price, the top three models were OpenAI's GPT 5 Chat (gpt-5-chat), Google's Gemini 2.5 Flash (gemini-2.5-flash), and Meta's Llama 4 Maverick (llama-4-maverick).
We settled on these three models as candidates for more rigorous downstream evaluative ablations to decide model deployment, following the industry practice of architecting systems that facilitate model switching \parencite{ng2025}.

\section{Frontend}

The frontend exposes the PDPA chatbot to SME users through a web interface and aims to minimise interaction friction.

We considered two user interface (UI) interaction modes: anonymous access and authenticated access.
Anonymous access requires no registration and lets users ask questions immediately, whereas
authenticated access requires registration and lets users retain preferences and past conversations through user accounts.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{images/frontend-full.png}
	\caption{Sketches of the frontend UI.}
\end{figure}

\vspacebaselineskip

Given SME needs and the scope of our minimum viable product, we adopt anonymous access to reduce barriers and shorten time-to-value, deferring account-based features to a later phase.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{Angular} & \textbf{React} & \textbf{Vue.js} \\
		\midrule
		\textbf{Interruptible reconciliation support?} & No & Yes & No \\
		\textbf{GitHub stars} & $\sim$100k & $\sim$240k & $\sim$210k \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between Angular, React, and Vue.js.}
\end{table}

\vspacebaselineskip

From our research, we found 3 frontend frameworks: Angular, React, and Vue.js.
We settled on React as
the support for interruptible reconciliation keeps interfaces responsive under load,
and the larger community support---as measured by the number of GitHub stars---facilitates debugging.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{Angular} & \textbf{React} & \textbf{Vue.js} \\
		\midrule
		\textbf{Interruptible reconciliation support?} & No & Yes & No \\
		\textbf{GitHub stars} & $\sim$100k & $\sim$240k & $\sim$210k \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between Angular, React, and Vue.js.}
\end{table}

\vspacebaselineskip

From our research, we found 2 full-stack React frameworks: Next.js and Remix.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}ll}
		\toprule
		& \textbf{Next.js} & \textbf{Remix} \\
		\midrule
		\textbf{Incremental static regeneration support?} & Yes & No \\
		\textbf{Built-in API route support?} & Yes & No \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between Next.js and Remix.}
\end{table}

\vspacebaselineskip

Here, API refers to ``application programming interface''.

We settled on Next.js as
the support for incremental static regeneration improves load performance, and
the support for built-in API routes simplifies server-side integration.

We design a modular component architecture to facilitate development.
Routing and layout reside in the main application layer, and reusable interface elements are factored into separate components.
Global state, including conversation history and UI configuration, is maintained in a central store built with Redux Toolkit.
This separation of concerns improves readability and supports future extensions.
Configuration is driven by environment variables that provide the backend API base uniform resource locator (URL) and related parameters.
This allows the same codebase to run in development, staging, and production without code changes.
To keep latency predictable and token usage bounded, the frontend limits each conversation to a fixed number of recent messages, discarding older turns from the request while retaining them in the on-screen history.

Given a user query, the frontend performs the following steps online.

\begin{enumerate}
	\item \textbf{Input capture.}
	The user message is added to the local conversation state and rendered immediately in the chat interface.
	\item \textbf{Request construction.}
	The frontend selects the most recent messages, then builds a request containing the conversation history, current query, retrieval settings, and model parameters.
	\item \textbf{Streaming consumption.}
	The frontend opens a streaming connection to the backend and incrementally reads server-sent events.
	Each content fragment is cleaned, converted from Markdown to sanitised Hypertext Markup Language (HTML), and merged into the latest assistant message in the state store.
	\item \textbf{Interface rendering.}
	As the state store updates, the interface re-renders the assistant message in real time, giving the impression of word-by-word generation.
	Conversation state, including errors, is persisted in browser storage, which supports multi-conversation views and future export features.
	\item \textbf{Error handling.}
	Network and parsing errors are caught and surfaced as concise messages in the chat, so failures degrade gracefully and remain visible to users and developers.
\end{enumerate}

\section{Infrastructure}

For deployment, the system requires a cloud platform that can host both the frontend and backend as containers, expose them securely to the internet, and scale with changes in user traffic.

From our research, we identified three cloud provider abstractions: Amazon Web Services, Microsoft Azure (Azure), and Google Cloud Platform (GCP).

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}lll}
		\toprule
		& \textbf{AWS} & \textbf{Azure} & \textbf{GCP} \\
		\midrule
		\textbf{Market share} & 29\% \parencite{statista2025} & 20\% \parencite{statista2025} & 13\% \parencite{statista2025} \\
		\textbf{Global region count} & 36 & 60 & 42 \\
		\textbf{Availability zone count} & 114 & 126 & 127 \\
		\textbf{Minimum service count} & 200 & 200 & 100 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison between AWS, Azure, and GCP.}
\end{table}

\vspacebaselineskip

AWS was selected for its large global footprint, mature support for containerised workloads, and close alignment with AsiaCloud's existing infrastructure, making it suitable for rapid prototyping and later production deployment.

Within AWS, we found six compute service abstractions from our research: Elastic Compute Cloud (EC2), Lambda, Fargate, App Runner, Lightsail, and Elastic Beanstalk.

\begin{table}[H]
	\centering
	\begin{tabular}{l@{\qquad}ll}
		\toprule
		& \textbf{Billing metric} & \textbf{Scaling metric} \\
		\midrule
		\textbf{EC2} & Instance-hours & Instances \\
		\textbf{Lambda} & Requests, GB-seconds & Executions \\
		\textbf{Fargate} & vCPU-hours, GB-hours & Tasks \\
		\textbf{App Runner} & vCPU-hours, GB-hours, requests & Instances \\
		\textbf{Lightsail} & Bundled instance-hours & Instances \\
		\textbf{Elastic Beanstalk} & Instance-hours, LCU-hours, GB-months & EC2 instances \\
		\bottomrule
	\end{tabular}
	\caption{Billing and scaling metrics for AWS services.}
\end{table}

\vspacebaselineskip

Here, vCPU refers to ``virtual central processing unit'' hours, GB refers to gigabyte, and LCU refers to ``load balancer capacity unit''.

To handle dynamic traffic, integrate with autoscaling, and support future features, AWS App Runner was chosen as the primary deployment service, allowing the number of running instances to grow or shrink with demand.

To deploy the frontend and backend, the system performs the following steps.

\begin{enumerate}
	\item \textbf{Application containerisation.} Docker builds separate container images for the frontend and backend, capturing their runtime dependencies.
	\item \textbf{Image storage.} Amazon Elastic Container Registry (ECR) stores the versioned images for deployment.
	\item \textbf{Service deployment.} App Runner pulls the images from ECR, provisions compute instances, and exposes the services over a secure protocol.
	\item \textbf{Operation scaling.} App Runner manages health checks, restarts failed instances, and performs basic autoscaling, which minimises operational overhead and lets us focus on application logic.
\end{enumerate}
